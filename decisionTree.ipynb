{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel, RandomForest\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time\n",
    "\n",
    "\n",
    "# Read the dataset that was previously splitted due to jupyter's maximum file size issue\n",
    "rawData = sc.textFile(\"*.csv\")\n",
    "def prepare(line):\n",
    "    values = line.split(',')\n",
    "    map(float, values)\n",
    "    featureVector = Vectors.dense(values[:-1])\n",
    "    # Decision tree labels varies from 0 to n-1\n",
    "    label = float(values[-1])-1\n",
    "    return LabeledPoint(label, featureVector)\n",
    "\n",
    "def new_prepare(line):\n",
    "    values = line.split(',')\n",
    "    map(float, values)\n",
    "    wilderness = values[10:14].index('1')\n",
    "    soil = values[14:54].index('1')\n",
    "    featureVector = Vectors.dense(values[0:10]+[wilderness,soil])\n",
    "    # Decision tree labels varies from 0 to n-1\n",
    "    label = float(values[-1])-1\n",
    "    return LabeledPoint(label, featureVector)\n",
    "\n",
    "data = rawData.map(lambda line : new_prepare(line))\n",
    "\n",
    "trainData, cvData, testData = data.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treino: trainData\n",
    "# Teste: cvData\n",
    "# Descobrir a configuração dos hyperparameters\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData, numClasses=7, categoricalFeaturesInfo={10: 4, 11: 40},\n",
    "    impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(cvData.map(lambda x: x.features))\n",
    "labelsAndPredictions = cvData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Verificar a precisão da Decision Tree usando o conjunto de teste\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData.union(cvData), numClasses=7, categoricalFeaturesInfo={}, \n",
    "                                     impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Random Forest com 20 Decision Trees\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "forest = RandomForest.trainClassifier(trainData.union(cvData), numClasses=7, categoricalFeaturesInfo={10: 4, 11: 40}, numTrees=20,\n",
    "                                      featureSubsetStrategy='auto', impurity='entropy', maxDepth=30, maxBins=300)\n",
    "predictions = forest.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions) \n",
    "k = MulticlassMetrics(labelsAndPredictions)\n",
    "print k.precision()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [11,12,11,7]\n",
    "# trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# model = trainer.fit(trainData)\n",
    "\n",
    "# result = model.transform(test)\n",
    "# xy = result.select(\"prediction\", \"label\")\n",
    "# evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "# print (\"Precision: \"+str(evaluator.evaluate(xy)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py 2)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
