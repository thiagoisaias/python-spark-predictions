{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[546] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector, Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel, RandomForest\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "import time\n",
    "\n",
    "\n",
    "# Read the dataset that was previously splitted due to jupyter's maximum file size issue\n",
    "rawData = sc.textFile(\"*.csv\")\n",
    "def prepare(line):\n",
    "    values = line.split(',')\n",
    "    map(float, values)\n",
    "    featureVector = Vectors.dense(values[:-1])\n",
    "    # Decision tree labels varies from 0 to n-1\n",
    "    label = float(values[-1])-1\n",
    "    return LabeledPoint(label, featureVector)\n",
    "\n",
    "def new_prepare(line):\n",
    "    values = line.split(',')\n",
    "    map(float, values)\n",
    "    wilderness = values[10:14].index('1')\n",
    "    soil = values[14:54].index('1')\n",
    "    featureVector = Vectors.dense(values[0:10]+[wilderness,soil])\n",
    "    # Decision tree labels varies from 0 to n-1\n",
    "    label = float(values[-1])-1\n",
    "    return LabeledPoint(label, featureVector)\n",
    "\n",
    "data = rawData.map(lambda line : new_prepare(line))\n",
    "\n",
    "trainData, cvData, testData = data.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treino: trainData\n",
    "# Teste: cvData\n",
    "# Descobrir a configuração dos hyperparameters\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData, numClasses=7, categoricalFeaturesInfo={10: 4, 11: 40},\n",
    "    impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(cvData.map(lambda x: x.features))\n",
    "labelsAndPredictions = cvData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/current/python/pyspark/mllib/evaluation.py:237: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923760088294\n",
      "--- 328.800930977 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Verificar a precisão da Decision Tree usando o conjunto de teste\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData.union(cvData), numClasses=7, categoricalFeaturesInfo={}, \n",
    "                                     impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Random Forest com 20 Decision Trees\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "forest = RandomForest.trainClassifier(trainData.union(cvData), numClasses=7, categoricalFeaturesInfo={10: 4, 11: 40}, numTrees=20,\n",
    "                                      featureSubsetStrategy='auto', impurity='entropy', maxDepth=30, maxBins=300)\n",
    "predictions = forest.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions) \n",
    "k = MulticlassMetrics(labelsAndPredictions)\n",
    "print k.precision()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.687774846086\n",
      "--- 329.418580055 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Multilayer Perceptron com 1 camada oculta\n",
    "\n",
    "# MLP não suporta feature categórica\n",
    "data = rawData.map(lambda line : prepare(line))\n",
    "\n",
    "trainData, cvData, testData = data.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainDF = trainData.union(cvData).map(lambda x: (x.label, Normalizer().transform(x.features).asML())).toDF([\"label\", \"features\"])\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[54, 100, 7], blockSize=128, seed=123)\n",
    "model = mlp.fit(trainDF)\n",
    "testDF = testData.map(lambda x: (Normalizer().transform(x.features).asML(),)).toDF([\"features\"])\n",
    "\n",
    "predictions = model.transform(testDF).select(\"prediction\")\n",
    "labelsAndPredictions = testData.map(lambda x: x.label).zip(predictions.rdd.map(lambda x: x[0])) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "print(m.precision())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py 2)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
