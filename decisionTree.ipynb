{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector, Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel, RandomForest\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData = sc.textFile(\"./datasets/covertype.data\")\n",
    "# rawData = sc.textFile(\"*.csv\")\n",
    "\n",
    "def prepare(line):\n",
    "    values = line.split(',')\n",
    "    map(float, values)\n",
    "    featureVector = Vectors.dense(values[:-1])\n",
    "    # Decision tree labels varies from 0 to n-1\n",
    "    label = float(values[-1])-1\n",
    "    return LabeledPoint(label, featureVector)\n",
    "\n",
    "def new_prepare(line):\n",
    "    values = line.split(',')\n",
    "    map(float, values)\n",
    "    wilderness = values[10:14].index('1')\n",
    "    soil = values[14:54].index('1')\n",
    "    featureVector = Vectors.dense(values[0:10]+[wilderness,soil])\n",
    "    # Decision tree labels varies from 0 to n-1\n",
    "    label = float(values[-1])-1\n",
    "    return LabeledPoint(label, featureVector)\n",
    "\n",
    "data = rawData.map(lambda line : new_prepare(line))\n",
    "\n",
    "trainData, cvData, testData = data.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698907394384\n",
      "--- 84.6671640873 seconds ---\n",
      "(0.6730000470300522, 0.6759246138585802)\n",
      "(0.7876652291294134, 0.7251686626424352)\n",
      "(0.8382227285673597, 0.6348144952545298)\n",
      "(0.425531914893617, 0.5128205128205128)\n",
      "(0.03143534994068802, 0.7464788732394366)\n",
      "(0.40645773979107314, 0.7074380165289256)\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData\n",
    "# Teste: cvData\n",
    "# Primeira Decision Tree, ainda não temos os hyperparemeters ideais\n",
    "\n",
    "first_data = rawData.map(lambda line : prepare(line))\n",
    "\n",
    "first_trainData, first_cvData, first_testData = first_data.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "first_trainData.cache()\n",
    "first_cvData.cache()\n",
    "first_testData.cache()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = DecisionTree.trainClassifier(first_trainData, numClasses=7, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=4, maxBins=100)\n",
    "predictions = model.predict(first_cvData.map(lambda x: x.features))\n",
    "labelsAndPredictions = first_cvData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "for category in range(7):\n",
    "    # não existe o label 5 no dataset\n",
    "    if category != 4:\n",
    "        print(m.precision(category), m.recall(category))\n",
    "        \n",
    "        \n",
    "# 0.702317136434\n",
    "# --- 72.0438899994 seconds ---\n",
    "\n",
    "# (0.6835904230150468, 0.684205541084537)\n",
    "# (0.7848303819074035, 0.7289210343802636)\n",
    "# (0.8460889138998312, 0.6264583333333333)\n",
    "# (0.3979591836734694, 0.5043103448275862)\n",
    "# (0.0, 0.0) não existe label 5\n",
    "# (0.03575547866205306, 0.7126436781609196)\n",
    "# (0.4487684729064039, 0.6917236142748672)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('gini', 1, 10), 0.6360193671104963)\n",
      "(('gini', 1, 300), 0.6350855957115684)\n",
      "(('gini', 20, 10), 0.8906277018848349)\n",
      "(('gini', 20, 300), 0.9035448729033374)\n",
      "(('entropy', 1, 10), 0.4862873940861145)\n",
      "(('entropy', 1, 300), 0.4862873940861145)\n",
      "(('entropy', 20, 10), 0.8960228255230849)\n",
      "(('entropy', 20, 300), 0.9099775203181739)\n",
      "--- 1304.84925485 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData\n",
    "# Teste: cvData\n",
    "# Descobrir a configuração dos hyperparameters\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "impurity = ['gini', 'entropy']\n",
    "depth = [1, 20]\n",
    "bins = [10, 300]\n",
    "\n",
    "for imp in impurity:\n",
    "    for dep in depth:\n",
    "        for b in bins:\n",
    "            model = DecisionTree.trainClassifier(first_trainData, numClasses=7, categoricalFeaturesInfo={},\n",
    "                                                 impurity=imp, maxDepth=dep, maxBins=b)\n",
    "            predictions = model.predict(first_cvData.map(lambda x: x.features))\n",
    "            labelsAndPredictions = first_cvData.map(lambda lp: lp.label).zip(predictions) \n",
    "            m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "            print((imp, dep, b), m.precision())\n",
    "            \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# (('gini', 1, 10), 0.6360193671104963)\n",
    "# (('gini', 1, 300), 0.6350855957115684)\n",
    "# (('gini', 20, 10), 0.8906277018848349)\n",
    "# (('gini', 20, 300), 0.9035448729033374)\n",
    "# (('entropy', 1, 10), 0.4862873940861145)\n",
    "# (('entropy', 1, 300), 0.4862873940861145)\n",
    "# (('entropy', 20, 10), 0.8960228255230849)\n",
    "# (('entropy', 20, 300), 0.9099775203181739)\n",
    "# --- 1304.84925485 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913047216204\n",
      "--- 153.932090998 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Verificar a precisão da Decision Tree usando o conjunto de teste\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = DecisionTree.trainClassifier(first_trainData.union(first_cvData), numClasses=7, categoricalFeaturesInfo={},\n",
    "                                     impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(first_testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = first_testData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 0.913047216204\n",
    "# --- 153.932090998 seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926365591398\n",
      "--- 186.282171965 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData\n",
    "# Teste: cvData\n",
    "# Descobrir a configuração dos hyperparameters com as features categóricas informadas\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData, numClasses=7, categoricalFeaturesInfo={10: 4, 11: 40},\n",
    "                                     impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(cvData.map(lambda x: x.features))\n",
    "labelsAndPredictions = cvData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 0.923581051717\n",
    "# --- 132.968389988 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926315789474\n",
      "--- 153.400055885 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Verificar a precisão da Decision Tree usando o conjunto de teste com as features categóricas informadas\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData.union(cvData), numClasses=7, categoricalFeaturesInfo={10: 4, 11: 40}, \n",
    "                                     impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 0.926020983265\n",
    "# --- 170.648295164 seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966402070751\n",
      "--- 4255.0600481 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Random Forest com 20 Decision Trees\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "forest = RandomForest.trainClassifier(trainData.union(cvData), numClasses=7, categoricalFeaturesInfo={10: 4, 11: 40}, numTrees=20,\n",
    "                                      featureSubsetStrategy='auto', impurity='entropy', maxDepth=30, maxBins=300)\n",
    "predictions = forest.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions) \n",
    "k = MulticlassMetrics(labelsAndPredictions)\n",
    "print k.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Utilizando maxBins=200\n",
    "# 0.948918123594\n",
    "# --- 2439.131598 seconds ---\n",
    "\n",
    "# Utilizando maxBins=300\n",
    "# 0.966402070751\n",
    "# --- 4255.0600481 seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "input = \"2709,125,28,67,23,3224,253,207,61,6094,0,29\"\n",
    "vector = input.split(',')\n",
    "map(float, vector)\n",
    "featureVector = Vectors.dense(vector)\n",
    "print(forest.predict(featureVfeature_selection.ipynbor))\n",
    "\n",
    "# 4.0 (Label 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Multilayer Perceptron com 1 camada oculta\n",
    "\n",
    "# MLP não suporta feature categórica\n",
    "data = rawData.map(lambda line : prepare(line))\n",
    "\n",
    "trainData, cvData, testData = data.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainDF = trainData.union(cvData).map(lambda x: (x.label, Normalizer().transform(x.features).asML())).toDF([\"label\", \"features\"])\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[54, 100, 7], blockSize=128, seed=123)\n",
    "model = mlp.fit(trainDF)\n",
    "testDF = testData.map(lambda x: (Normalizer().transform(x.features).asML(),)).toDF([\"features\"])\n",
    "\n",
    "predictions = model.transform(testDF).select(\"prediction\")\n",
    "labelsAndPredictions = testData.map(lambda x: x.label).zip(predictions.rdd.map(lambda x: x[0])) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "print(m.precision())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 0.687774846086\n",
    "# --- 329.418580055 seconds ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py 2)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
