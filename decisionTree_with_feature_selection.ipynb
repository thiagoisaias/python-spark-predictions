{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[197] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector, Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel, RandomForest\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "import time\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Read the dataset that was previously splitted due to jupyter's maximum file size issue\n",
    "rawData = sc.textFile(\"./datasets/covertype.data\")\n",
    "\n",
    "parsedData = rawData.map(lambda line: [int(x) for x in line.split(',')])\n",
    "\n",
    "def funcao1(line):\n",
    "    newarray = []\n",
    "    qwerty = line[3]**2 + line[4]**2\n",
    "    qwerty = int(sqrt(qwerty))\n",
    "    newarray.extend([line[0],qwerty,line[5],line[9]])\n",
    "    #newarray.extend([qwerty])\n",
    "    #newarray.extend(line[5:10])\n",
    "    #i = 1\n",
    "    #for j in range(10,14):\n",
    "    #    if line[j] == 1:\n",
    "    #        newarray.extend([i])\n",
    "    #    else:\n",
    "    #        i = i + 1\n",
    "    i = 1\n",
    "    for j in range(14,54):\n",
    "        if line[j] == 1:\n",
    "            newarray.extend([i])\n",
    "        else:\n",
    "            i = i + 1\n",
    "    newarray.extend([line[54]])\n",
    "    return newarray\n",
    "        \n",
    "\n",
    "parsedData2 = parsedData.map(funcao1)\n",
    "parsedData2.persist()\n",
    "\n",
    "\n",
    "def prepare(line):\n",
    "    #values = line.split(',')\n",
    "    values = line\n",
    "    map(float, values)\n",
    "    featureVector = Vectors.dense(values[:-1])\n",
    "    # Decision tree labels varies from 0 to n-1\n",
    "    label = float(values[-1])-1\n",
    "    return LabeledPoint(label, featureVector)\n",
    "\n",
    "def new_prepare(line):\n",
    "    #values = line.split(',')\n",
    "    values = line\n",
    "    map(float, values)\n",
    "    wilderness = values[9]-1\n",
    "    soil = values[10]-1\n",
    "    featureVector = Vectors.dense(values[0:9]+[wilderness,soil])\n",
    "    # Decision tree labels varies from 0 to n-1\n",
    "    label = float(values[-1])-1\n",
    "    return LabeledPoint(label, featureVector)\n",
    "\n",
    "data = parsedData2.map(lambda line : prepare(line))\n",
    "\n",
    "trainData, cvData, testData = data.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916072896684\n",
      "--- 93.2208759785 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData\n",
    "# Teste: cvData\n",
    "# Descobrir a configuração dos hyperparameters\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData, numClasses=7, categoricalFeaturesInfo={},\n",
    "    impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(cvData.map(lambda x: x.features))\n",
    "labelsAndPredictions = cvData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 0.916072896684\n",
    "# --- 93.2208759785 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915808414018\n",
      "--- 102.161608934 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Verificar a precisão da Decision Tree usando o conjunto de teste\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainData.union(cvData), numClasses=7, categoricalFeaturesInfo={}, \n",
    "                                     impurity='entropy', maxDepth=20, maxBins=300)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "\n",
    "print m.precision()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 0.915808414018\n",
    "# --- 102.161608934 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Random Forest com 20 Decision Trees\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "forest = RandomForest.trainClassifier(trainData.union(cvData), numClasses=7, categoricalFeaturesInfo={}, numTrees=20,\n",
    "                                      featureSubsetStrategy='auto', impurity='entropy', maxDepth=30, maxBins=300)\n",
    "predictions = forest.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions) \n",
    "k = MulticlassMetrics(labelsAndPredictions)\n",
    "print k.precision()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Treino: trainData + cvData\n",
    "# Teste: testData\n",
    "# Multilayer Perceptron com 1 camada oculta\n",
    "\n",
    "# MLP não suporta feature categórica\n",
    "data = rawData.map(lambda line : prepare(line))\n",
    "\n",
    "trainData, cvData, testData = data.randomSplit([0.8, 0.1, 0.1])\n",
    "\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainDF = trainData.union(cvData).map(lambda x: (x.label, Normalizer().transform(x.features).asML())).toDF([\"label\", \"features\"])\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[54, 100, 7], blockSize=128, seed=123)\n",
    "model = mlp.fit(trainDF)\n",
    "testDF = testData.map(lambda x: (Normalizer().transform(x.features).asML(),)).toDF([\"features\"])\n",
    "\n",
    "predictions = model.transform(testDF).select(\"prediction\")\n",
    "labelsAndPredictions = testData.map(lambda x: x.label).zip(predictions.rdd.map(lambda x: x[0])) \n",
    "m = MulticlassMetrics(labelsAndPredictions)\n",
    "print(m.precision())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py 2)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
